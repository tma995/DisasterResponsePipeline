{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mtx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mtx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mtx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "2        0      0            0             0                 0      ...         \n",
       "3        1      0            1             0                 1      ...         \n",
       "4        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "#df = pd.read_sql(\"SELECT * FROM InsertTableName\", engine)\n",
    "df = pd.read_sql_table('MessageAndLabel',engine)\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid_centers': 0,\n",
       " 'aid_related': 0,\n",
       " 'buildings': 0,\n",
       " 'clothing': 0,\n",
       " 'cold': 0,\n",
       " 'death': 0,\n",
       " 'direct_report': 1,\n",
       " 'earthquake': 0,\n",
       " 'electricity': 0,\n",
       " 'fire': 0,\n",
       " 'floods': 0,\n",
       " 'food': 0,\n",
       " 'genre': 'direct',\n",
       " 'hospitals': 0,\n",
       " 'id': 631,\n",
       " 'infrastructure_related': 0,\n",
       " 'medical_help': 0,\n",
       " 'medical_products': 0,\n",
       " 'message': 'There are so many people who came to city of Les Cayes, they have nowhere to stay.',\n",
       " 'military': 0,\n",
       " 'missing_people': 0,\n",
       " 'money': 0,\n",
       " 'offer': 0,\n",
       " 'original': 'okay gen anpil ki soti nan kapital ki moun okay yo pa gen okenn kot',\n",
       " 'other_aid': 0,\n",
       " 'other_infrastructure': 0,\n",
       " 'other_weather': 0,\n",
       " 'refugees': 0,\n",
       " 'related': 1,\n",
       " 'request': 0,\n",
       " 'search_and_rescue': 0,\n",
       " 'security': 0,\n",
       " 'shelter': 0,\n",
       " 'shops': 0,\n",
       " 'storm': 0,\n",
       " 'tools': 0,\n",
       " 'transport': 0,\n",
       " 'water': 0,\n",
       " 'weather_related': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[511].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline_eval(Y_true,Y_pred):\n",
    "    tmp_list = []\n",
    "    for i,c in enumerate(Y_true.columns):\n",
    "        accuracy = accuracy_score(Y_true[c], Y_pred[:,i])\n",
    "        precision = precision_score(Y_true[c], Y_pred[:,i])\n",
    "        recall = recall_score(Y_true[c], Y_pred[:,i])\n",
    "        f1 = f1_score(Y_true[c], Y_pred[:,i])\n",
    "        tmp_dict = {'feature':c,\n",
    "                    'accuracy':accuracy,\n",
    "                    'precision':precision,\n",
    "                    'recall':recall,\n",
    "                    'f1':f1\n",
    "                   }\n",
    "        tmp_list.append(tmp_dict)\n",
    "        \n",
    "    return pd.DataFrame(tmp_list).set_index('feature').sort_values('f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.992777</td>\n",
       "      <td>0.995299</td>\n",
       "      <td>0.993345</td>\n",
       "      <td>0.997261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.984734</td>\n",
       "      <td>0.981431</td>\n",
       "      <td>0.996583</td>\n",
       "      <td>0.966732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.987859</td>\n",
       "      <td>0.977976</td>\n",
       "      <td>0.996969</td>\n",
       "      <td>0.959694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.995799</td>\n",
       "      <td>0.977398</td>\n",
       "      <td>0.996627</td>\n",
       "      <td>0.958897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.994570</td>\n",
       "      <td>0.970056</td>\n",
       "      <td>0.997676</td>\n",
       "      <td>0.943925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.969967</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.942974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.987962</td>\n",
       "      <td>0.963896</td>\n",
       "      <td>0.999363</td>\n",
       "      <td>0.930861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.992418</td>\n",
       "      <td>0.954960</td>\n",
       "      <td>0.999363</td>\n",
       "      <td>0.914336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.983146</td>\n",
       "      <td>0.954577</td>\n",
       "      <td>0.997691</td>\n",
       "      <td>0.915034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.991701</td>\n",
       "      <td>0.947437</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>0.900679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.993136</td>\n",
       "      <td>0.943027</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.892915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.998105</td>\n",
       "      <td>0.935201</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.994263</td>\n",
       "      <td>0.933649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.988167</td>\n",
       "      <td>0.919203</td>\n",
       "      <td>0.997722</td>\n",
       "      <td>0.852140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.995031</td>\n",
       "      <td>0.917447</td>\n",
       "      <td>0.998148</td>\n",
       "      <td>0.848819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.979202</td>\n",
       "      <td>0.915768</td>\n",
       "      <td>0.999547</td>\n",
       "      <td>0.844946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.992162</td>\n",
       "      <td>0.914190</td>\n",
       "      <td>0.997552</td>\n",
       "      <td>0.843685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.992060</td>\n",
       "      <td>0.913745</td>\n",
       "      <td>0.997570</td>\n",
       "      <td>0.842916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.998975</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.996260</td>\n",
       "      <td>0.900950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.993802</td>\n",
       "      <td>0.900575</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.821589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.998053</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.997746</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.808696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.989703</td>\n",
       "      <td>0.894043</td>\n",
       "      <td>0.996475</td>\n",
       "      <td>0.810707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.991035</td>\n",
       "      <td>0.893097</td>\n",
       "      <td>0.997271</td>\n",
       "      <td>0.808628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.995492</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.985503</td>\n",
       "      <td>0.876365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.997541</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.989755</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.993341</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.997525</td>\n",
       "      <td>0.757519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.995594</td>\n",
       "      <td>0.854237</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.996721</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.994286</td>\n",
       "      <td>0.734177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.998822</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.998412</td>\n",
       "      <td>0.797386</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy        f1  precision    recall\n",
       "feature                                                        \n",
       "related                 0.992777  0.995299   0.993345  0.997261\n",
       "aid_related             0.984734  0.981431   0.996583  0.966732\n",
       "weather_related         0.987859  0.977976   0.996969  0.959694\n",
       "earthquake              0.995799  0.977398   0.996627  0.958897\n",
       "storm                   0.994570  0.970056   0.997676  0.943925\n",
       "food                    0.993443  0.969967   0.998551  0.942974\n",
       "request                 0.987962  0.963896   0.999363  0.930861\n",
       "shelter                 0.992418  0.954960   0.999363  0.914336\n",
       "direct_report           0.983146  0.954577   0.997691  0.915034\n",
       "floods                  0.991701  0.947437   0.999316  0.900679\n",
       "water                   0.993136  0.943027   0.999099  0.892915\n",
       "clothing                0.998105  0.935201   1.000000  0.878289\n",
       "death                   0.994263  0.933649   1.000000  0.875556\n",
       "cold                    0.997234  0.925620   1.000000  0.861538\n",
       "medical_help            0.988167  0.919203   0.997722  0.852140\n",
       "military                0.995031  0.917447   0.998148  0.848819\n",
       "other_aid               0.979202  0.915768   0.999547  0.844946\n",
       "medical_products        0.992162  0.914190   0.997552  0.843685\n",
       "buildings               0.992060  0.913745   0.997570  0.842916\n",
       "tools                   0.998975  0.908257   1.000000  0.831933\n",
       "electricity             0.996260  0.900950   1.000000  0.819753\n",
       "refugees                0.993802  0.900575   0.996364  0.821589\n",
       "fire                    0.998053  0.900524   1.000000  0.819048\n",
       "missing_people          0.997746  0.894231   1.000000  0.808696\n",
       "other_weather           0.989703  0.894043   0.996475  0.810707\n",
       "transport               0.991035  0.893097   0.997271  0.808628\n",
       "money                   0.995492  0.887755   1.000000  0.798165\n",
       "infrastructure_related  0.985503  0.876365   1.000000  0.779938\n",
       "hospitals               0.997541  0.872340   1.000000  0.773585\n",
       "other_infrastructure    0.989755  0.870130   1.000000  0.770115\n",
       "search_and_rescue       0.993341  0.861111   0.997525  0.757519\n",
       "security                0.995594  0.854237   1.000000  0.745562\n",
       "aid_centers             0.996721  0.844660   0.994286  0.734177\n",
       "shops                   0.998822  0.843537   1.000000  0.729412\n",
       "offer                   0.998412  0.797386   1.000000  0.663043"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred = pipeline.predict(X_train)\n",
    "eval_train = pipeline_eval(Y_train,Y_train_pred)\n",
    "eval_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.992872</td>\n",
       "      <td>0.993443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.914573</td>\n",
       "      <td>0.914190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.998487</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.846945</td>\n",
       "      <td>0.843685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       50%\n",
       "accuracy   0.992872  0.993443\n",
       "f1         0.914573  0.914190\n",
       "precision  0.998487  0.999316\n",
       "recall     0.846945  0.843685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_train.describe().T[['mean','50%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtx/anaconda/envs/dlnd-tf-lab/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mtx/anaconda/envs/dlnd-tf-lab/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.939652</td>\n",
       "      <td>0.956201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.215394</td>\n",
       "      <td>0.128440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.589990</td>\n",
       "      <td>0.727848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.158733</td>\n",
       "      <td>0.070707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       50%\n",
       "accuracy   0.939652  0.956201\n",
       "f1         0.215394  0.128440\n",
       "precision  0.589990  0.727848\n",
       "recall     0.158733  0.070707"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = pipeline.predict(X_test)\n",
    "eval0 = pipeline_eval(Y_test,Y_test_pred)\n",
    "eval0.describe().T[['mean','50%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__n_jobs': 1,\n",
       " 'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x115da70d0>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x115da70d0>, vocabulary=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize>,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__max_depth':[3, 5],\n",
    "              'clf__estimator__max_features': ['sqrt', 0.3]\n",
    "             }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_model0 = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 10.15157406,   9.94672736,  11.73424967,  12.18626428,\n",
       "         17.43695339,  15.37913744,  30.41821798,  27.30893532,\n",
       "         10.1203297 ,  10.38176966,  12.33035501,  11.62182132,\n",
       "         22.2277511 ,  18.79600739,  43.25027196,  35.03502162]),\n",
       " 'mean_score_time': array([ 4.4843262 ,  4.43246698,  4.97545727,  4.95361463,  4.48345868,\n",
       "         4.31890329,  4.97278333,  4.79757675,  4.40710425,  4.4726487 ,\n",
       "         5.66926138,  4.3980213 ,  4.15895597,  4.10800934,  4.61076204,\n",
       "         4.43337488]),\n",
       " 'mean_test_score': array([ 0.19891399,  0.19788945,  0.19978485,  0.19942626,  0.2004508 ,\n",
       "         0.19799191,  0.21689463,  0.21264279,  0.19799191,  0.19256186,\n",
       "         0.19891399,  0.19840172,  0.21668972,  0.22247836,  0.2258081 ,\n",
       "         0.22616669]),\n",
       " 'mean_train_score': array([ 0.19909328,  0.19814559,  0.19981046,  0.19957994,  0.20862148,\n",
       "         0.20547103,  0.22414323,  0.22045489,  0.19878592,  0.19412428,\n",
       "         0.19927258,  0.19886276,  0.24204703,  0.24704165,  0.25236924,\n",
       "         0.25119103]),\n",
       " 'param_clf__estimator__max_depth': masked_array(data = [3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__max_features': masked_array(data = ['sqrt' 'sqrt' 'sqrt' 'sqrt' 0.3 0.3 0.3 0.3 'sqrt' 'sqrt' 'sqrt' 'sqrt'\n",
       "  0.3 0.3 0.3 0.3],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 25 25 10 10 25 25 10 10 25 25 10 10 25 25],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__min_df': masked_array(data = [1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 3,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 'sqrt',\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__max_features': 0.3,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'vect__min_df': 5}],\n",
       " 'rank_test_score': array([10, 15,  8,  9,  7, 13,  4,  6, 13, 16, 10, 12,  5,  3,  2,  1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.19609651,  0.19609651,  0.1970186 ,  0.19686491,  0.20685416,\n",
       "         0.18472414,  0.20454895,  0.19394498,  0.19655755,  0.19117873,\n",
       "         0.1970186 ,  0.19548179,  0.21254034,  0.21008145,  0.21868757,\n",
       "         0.21622868]),\n",
       " 'split0_train_score': array([ 0.20055325,  0.20078377,  0.20116797,  0.20109113,  0.22352851,\n",
       "         0.19863224,  0.21814968,  0.21115722,  0.20070693,  0.19932381,\n",
       "         0.20116797,  0.19986169,  0.24604272,  0.24281543,  0.24988474,\n",
       "         0.24880897]),\n",
       " 'split1_test_score': array([ 0.19394498,  0.19179345,  0.1945597 ,  0.19363762,  0.19993853,\n",
       "         0.20393422,  0.21715076,  0.21699708,  0.19133241,  0.18641463,\n",
       "         0.1930229 ,  0.19486707,  0.2128477 ,  0.22422007,  0.2223759 ,\n",
       "         0.22452743]),\n",
       " 'split1_train_score': array([ 0.20201322,  0.20047641,  0.20239742,  0.20201322,  0.21184878,\n",
       "         0.21776548,  0.23282619,  0.22959889,  0.20093745,  0.19425234,\n",
       "         0.20162901,  0.20278162,  0.24711849,  0.25426464,  0.25818349,\n",
       "         0.25672353]),\n",
       " 'split2_test_score': array([ 0.20670048,  0.20577839,  0.20777624,  0.20777624,  0.1945597 ,\n",
       "         0.20531735,  0.22898417,  0.22698632,  0.20608575,  0.20009221,\n",
       "         0.20670048,  0.20485631,  0.22468111,  0.23313355,  0.23636084,\n",
       "         0.23774397]),\n",
       " 'split2_train_score': array([ 0.19471339,  0.19317658,  0.19586599,  0.19563547,  0.19048717,\n",
       "         0.20001537,  0.22145382,  0.22060858,  0.19471339,  0.18879668,\n",
       "         0.19502075,  0.19394498,  0.23297987,  0.24404487,  0.2490395 ,\n",
       "         0.24804057]),\n",
       " 'std_fit_time': array([ 0.25660001,  0.72503896,  0.50923751,  0.45920563,  0.40756494,\n",
       "         0.5345117 ,  1.20202957,  1.43434174,  0.65272687,  0.18169986,\n",
       "         0.62748679,  0.34091646,  0.19940687,  0.04614841,  0.3772451 ,\n",
       "         0.16741625]),\n",
       " 'std_score_time': array([ 0.01835371,  0.29432325,  0.09864058,  0.26699415,  0.20109856,\n",
       "         0.26321055,  0.33498456,  0.29025799,  0.24810442,  0.03134347,\n",
       "         0.67271781,  0.03734809,  0.07634227,  0.06881184,  0.09824215,\n",
       "         0.06648784]),\n",
       " 'std_test_score': array([ 0.0055755 ,  0.0058484 ,  0.00573924,  0.00604955,  0.00503224,\n",
       "         0.0093987 ,  0.00997728,  0.013836  ,  0.00610783,  0.00566885,\n",
       "         0.00574244,  0.00457098,  0.00565216,  0.00949122,  0.00761232,\n",
       "         0.00885973]),\n",
       " 'std_train_score': array([ 0.00315389,  0.00351586,  0.00283396,  0.00281445,  0.01368074,\n",
       "         0.00871181,  0.00628621,  0.00752957,  0.00288126,  0.00429863,\n",
       "         0.00301238,  0.00367604,  0.00642647,  0.00513203,  0.00412575,\n",
       "         0.00392463])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model0.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__max_depth': 5,\n",
       " 'clf__estimator__max_features': 0.3,\n",
       " 'clf__estimator__n_estimators': 25,\n",
       " 'vect__min_df': 5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model0.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtx/anaconda/envs/dlnd-tf-lab/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mtx/anaconda/envs/dlnd-tf-lab/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.943270</td>\n",
       "      <td>0.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.350515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.650716</td>\n",
       "      <td>0.745763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.269460</td>\n",
       "      <td>0.228412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       50%\n",
       "accuracy   0.943270  0.964500\n",
       "f1         0.342871  0.350515\n",
       "precision  0.650716  0.745763\n",
       "recall     0.269460  0.228412"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = cv_model0.predict(X_test)\n",
    "eval1 = pipeline_eval(Y_test,Y_test_pred)\n",
    "eval1.describe().T[['mean','50%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cv_model0, open('disaster_response_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disaster_response_model.m']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(cv_model0, 'disaster_response_model.m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
